
History of parallel computing:

Traditionally, computer software has been written to be computed serially normally described by the bit-serial architecture where the processor operators on one bit or digit for each clock cycle. To solve a given problem, a algorithm would be constructed and implemented as a serial stream of instructions. These instructions are executed by a CPU via the fetch-decode-execute cycle. 

Parallel computing, uses multiple processing elements simultaneously to solve a given problem. This is accomplished by deconstructing the problem into independant parts that each processing element can execute simultaneously with others. These processing elements can be anything such as a single computer with multiple precssors, networked computers or specialized hardware. Historically this was used for scientific computing and simulation of scientific problems such as weather systems etc. This then led to parallel hardware, software and high-performance computing.

Before the implementation of parallel computing. Computer perfomance improved via frequency scaling where newer processors would just have higher clock speeds, this increased computer performance as the average runtime of a program is the number of instructions multiplied by the average runtime of the instruction. So by doing more instructions a second, the average runtime of a program would decrease. But the issue of power consumption started to rise, as the power consumption of a given processor is given by the product of the capactitance switched by each clock cycle, the frequency and the square of the voltage, increasing the frequency increased the power consumption. 

The reason why power consumption was such an issue is because of the technology of the time, the heat transfer from the CPU to the surrounding atmosphere was not as advanced as it is today and resulting in the CPU overheating and either breaking or thermal throttling, where the CPU would slow down its clock speed to reduce the amount of power consumed defeating the purpose of operating at a higher frequency in the first place. 

To solve this problem processor manufacturers changed their focus from clock speed to having mulitple cores that can act independantly of each other and acces the systems memory concurrently, bringing parallel computing to desktop computers. With the hardware sorted out, the software of a computer had to change as well to be able to support this. And so operating systems were built to ensure that different tasks and user programs are run in parrlel on the available cores. But previous computer software which had been writted for serial processors are not optimised to run on multi-core architectures and so the developers have to parallelise their software code to take advantage of the increasing computing power of multicore architectures.

Parallel Computing:

When it comes to parallel computing there are three types of parallelism; Bit-level prallelism, Instruction-level parallelism and Task pallelism.

Bit-level Parallelism:
This is a form of parallel computing basded on increasing the processor word size, a word is the natrual unit of data used by a particular processor design which is a fixed-size piece of data handled as a single unit by the instrcution set of the processor. Increasing the word size, reduces the number of instructioins that the processor must execute in order to perform a given operation on variables that are larger than the length of the word. 
For Example: Lets say you have an 8-bit processor that needs to add 16-bit integers. It would have to add the 8 lower-order bits from each integer, then add the 8 higher-order bits. But an 16-bit proccesor can do it in one operation. So by increasing the wordsize of a processor you can reduce instructions  required to carry out operations.

Instruction-level parallelism:
As a computer program is essentially a stream of instructions that a processor carries out, processors that dont utilise instruction-level parallism can only issue less that one instruction per clock cycle (these processors are known as sub-scalar processors). But with instruction-level parallism these instructions can be re-ordered and combined into groups with each group executed in parallel, without changing the result of the program. All modern processsors have multi-stage instruction pipelines which are a technique to implement instruction-level parallelism within a single processor.

https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Fivestagespipeline.png/300px-Fivestagespipeline.png

Each stage in a multi-stage instruction pipeline coreesponds to a different action the processor performs on that instruction in that stage. Beacuse of this processors can issue one instruction per clock cycle and are known as scalar processors. And as most modern processors have multiple cores (execution units) they can combine these cores with pipelining and can issue more than one instruction per clock cycle. These processors are known as superscalar processors. 

With instruction-level parallelism instructions can only be grouped together if they have no data dependency between them. So algorithms such as the Tomasulo and Scoreboarding algorithm are utilised to implement out-of-order execution and instruciton-level parallelism.

Task Prallelism:
Task pallelism is a characteristic of a parallel program that can perfrom entirely different calculations on either the same or different sets of data. Which contrasts with data pallelism, in which the same calculation is performed on the same or different sets of data. Task pallelism invovles the decompostion of a task into sub-tasks and then allocating each sub-task to a processor for execution. These sub-tasks are then executed concurrently and cooperatively. But task parallism does not usually scale with the size of a problem.

What is meant by a parallel System?

Parallel processing systems are systems deisgned to speed up program execution via the above pallelism methods, mainly instruction-level parallelism. 
These systems deal with the simultaneous use of multiple computer resources such as single computers with multiple processors or networked computers, however the later is normally only seen at an enterprise level due to cost and complexity. Wheras the former is usually seen at a consumer level in desktop computers and mobile devices like smartphones and tablets. 

Parallel Systems have the benefit of being faster than serial systems since there are multiple processors working on a task at once, ease of scaleability, a higher throughput of data and increased reliability as if a processor is lost the system can continue albeit at a slower speed. However the implementation of a parallel system is difficult due to the extraction of the parallelism in a problem (Ahhmdals Law) and writing a program to be executed parallely is difficult.

Multicore Systems:

Multicore Systems are now everywhere and pretty much every computational device has more than one core (except in special cases such as embedded systems etc.), they are advantageous in parallel processing thanks to the CPU having multiple cores that can process tasks individually acheiving superscalar perfomance. Multicore systems also give increased performance as the operating system can allocate tasks to the cpu to always keep it busy making the system feel more responsive and snappy.

Multicore systems can also truly execute multiplet asks concurrently unlike singlecore systems which can schedule tasks to give the perception that the system is multitasking when in reality its just executing tasks serially. Multicore systems achieve this by having the operating system split the different applications / precoesses between the seperate CPU cores with each core either executing the program serially or justing instruction-level parallism. 

Some applications are resource intensive so having a multi-core system then application can run seperate sub-processes on different cores to speed the process up in general, this is particullaly advantageous in rendering applications where a GPU cannot be used. Such as batch-rendering in programs such as lightoom. And web-browsers such as google chrome which will spawn new threads for every tab opened, so having more threads available (multicore systems) results in the application acting more responsive when switching between tabs. 

Sources:
https://en.wikipedia.org/wiki/Parallel_computing
https://en.wikipedia.org/wiki/Bit-level_parallelism
http://ecomputernotes.com/fundamental/disk-operating-system/parallel-processing-systems
https://en.wikipedia.org/wiki/Multi-core_processor